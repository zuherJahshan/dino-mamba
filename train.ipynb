{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import get_dataloader\n",
    "from fairseq.models import BaseFairseqModel\n",
    "from fairseq.models.wav2vec import (\n",
    "    TransformerEncoder,\n",
    ")\n",
    "import torch\n",
    "import multiprocessing as mp\n",
    "\n",
    "from dinosr import DinoSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the multiprocessing start method to 'spawn'\n",
    "mp.set_start_method('spawn', force=True)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you have a series of n convolution layers, each convolution, i, has k_i, s_i, as a kernel and a stride. The output of the ith layer is given by:\n",
    "$$ x_{i+1} = \\frac{x_i - k_i}{s_i} $$\n",
    "Also, itcan be re-written as:\n",
    "$$ s_i x_{i+1} = x_i - k_i $$\n",
    "where\n",
    "$$ s_{i-1} x_i = x_{i-1} - k_{i-1} $$\n",
    "This implies that:\n",
    "$$ s_i s_{i-1} x_{i+1} = x_{i-1} - s_{i-1}k_i - k_{i-1}$$\n",
    "\n",
    "And from here we can derive a general formula for the output of the nth layer, as a function of the input of first layer:\n",
    "$$\n",
    "\\Pi_{i=1}^{n-1}{s_i} x_n = x_1 - \\sum_{i=1}^{n-1}{\\Pi_{j=i+1}^{n-1}{s_j}k_i}\n",
    "$$\n",
    "\n",
    "Note that the Sigma-Pi is not input dependent and hence, it is a constant depending only on the network architecture. This is an important quality, since it makes the computation of the relevant output window an O(1) operation.\n",
    "\n",
    "To deal with boundries, we can write the following formula:\n",
    "$$\n",
    "\\alpha = \\Pi_{i=1}^{n-1}{s_i} \\\\\n",
    "\\beta = \\sum_{i=1}^{n-1}{\\Pi_{j=i+1}^{n-1}{s_j}k_i} \\\\\n",
    "x_n \\ge \\lceil \\frac{x_1 - \\beta}{\\alpha} \\rceil\n",
    "$$\n",
    "where $\\alpha$ and $\\beta$ are constants depending only on the network architecture. Noting that x_n is almost equal to $\\lceil \\frac{x_1 - \\beta}{\\alpha} \\rceil$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dinosr_config import DinosrAudioConfig\n",
    "\n",
    "cfg = DinosrAudioConfig(average_top_k_layers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_creator(cfg):\n",
    "    return TransformerEncoder(cfg)\n",
    "\n",
    "dino_model = DinoSR(cfg, model_creator).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_persistant_state import ModelPersistantState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no model to load\n"
     ]
    }
   ],
   "source": [
    "model_persistant_state = ModelPersistantState('./models/dino_transformer_model')\n",
    "try:\n",
    "    model_persistant_state.load_model(dino_model)\n",
    "    print(\"loaded model successfully\")\n",
    "except:\n",
    "    print(\"no model to load\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "num_epochs = 1\n",
    "batch_size = 320\n",
    "mini_batch_size = 16\n",
    "learning_rate = 0.0005\n",
    "\n",
    "# Define the learning rate schedule\n",
    "def lr_lambda(initial_step, step):\n",
    "    warmup_steps = 12000\n",
    "    hold_steps = 188000\n",
    "    decay_steps = 200000\n",
    "    initial_lr = 0.0005\n",
    "    final_lr = 0.00005\n",
    "\n",
    "    modified_step = step + initial_step\n",
    "\n",
    "    if modified_step < warmup_steps:\n",
    "        return modified_step / warmup_steps\n",
    "    elif modified_step < warmup_steps + hold_steps:\n",
    "        return 1.0\n",
    "    else:\n",
    "        decay_factor = (modified_step - (warmup_steps + hold_steps)) / decay_steps\n",
    "        return initial_lr * ((final_lr / initial_lr) ** decay_factor) / initial_lr\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(dino_model1.parameters(), lr=0.0005)\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda step: lr_lambda(model_persistant_state.get_current_step(), step))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = get_dataloader(mini_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "# Function to save training history to a YAML file\n",
    "def save_training_history(history, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        yaml.dump(history, file)\n",
    "\n",
    "total_step = len(trainset)\n",
    "n = batch_size // mini_batch_size  # Update parameters every n batches\n",
    "\n",
    "batch_step = model_persistant_state.get_current_step()\n",
    "batches_to_step = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_accuracy = 0.0\n",
    "    for i, (waveforms, lengths) in enumerate(trainset):\n",
    "        step = epoch * total_step + i  # Calculate the current step\n",
    "\n",
    "        # Forward pass\n",
    "        results = dino_model(waveforms, lengths)\n",
    "        loss = results['loss'] / n\n",
    "        accuracy = results['accuracy']\n",
    "        \n",
    "        # Accumulate loss and accuracy\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_accuracy += accuracy.item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Accumulate gradients and update parameters\n",
    "        if (i + 1) % n == 0 or (i + 1) == total_step:\n",
    "            # Increment the batch step\n",
    "            batch_step += 1\n",
    "            \n",
    "            optimizer.step()\n",
    "            dino_model.update_teacher_params(batch_step=batch_step)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Step the scheduler\n",
    "            scheduler.step()\n",
    "\n",
    "            # Save the model and training history\n",
    "            model_persistant_state.save_model(\n",
    "                step=batch_step,\n",
    "                model=dino_model,\n",
    "                performance={\n",
    "                    'loss': epoch_loss / (i + 1),\n",
    "                    'accuracy': epoch_accuracy / (i + 1)\n",
    "                }\n",
    "            )            \n",
    "\n",
    "            print(f'\\rEpoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{total_step}], Loss: {epoch_loss / (i + 1):.4f}, Accuracy: {100 * epoch_accuracy / (i + 1):.2f}% local accuracy is {100 * accuracy:.2f}%, and local loss is: {loss:.4f}', end='', flush=True)\n",
    "            if batch_step == batches_to_step:\n",
    "                break\n",
    "\n",
    "    # Calculate and print cumulative loss and accuracy for the epoch\n",
    "    avg_loss = epoch_loss / total_step\n",
    "    avg_accuracy = epoch_accuracy / total_step\n",
    "    print(f'\\nEpoch [{epoch + 1}/{num_epochs}] Summary: Avg Loss: {avg_loss:.4f}, Avg Accuracy: {100 * avg_accuracy:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
