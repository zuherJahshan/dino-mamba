{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import get_dataloader\n",
    "from fairseq.models import BaseFairseqModel\n",
    "from fairseq.models.wav2vec import (\n",
    "    TransformerEncoder,\n",
    ")\n",
    "import torch\n",
    "import multiprocessing as mp\n",
    "\n",
    "from dinosr import DinoSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the multiprocessing start method to 'spawn'\n",
    "mp.set_start_method('spawn', force=True)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you have a series of n convolution layers, each convolution, i, has k_i, s_i, as a kernel and a stride. The output of the ith layer is given by:\n",
    "$$ x_{i+1} = \\frac{x_i - k_i}{s_i} $$\n",
    "Also, itcan be re-written as:\n",
    "$$ s_i x_{i+1} = x_i - k_i $$\n",
    "where\n",
    "$$ s_{i-1} x_i = x_{i-1} - k_{i-1} $$\n",
    "This implies that:\n",
    "$$ s_i s_{i-1} x_{i+1} = x_{i-1} - s_{i-1}k_i - k_{i-1}$$\n",
    "\n",
    "And from here we can derive a general formula for the output of the nth layer, as a function of the input of first layer:\n",
    "$$\n",
    "\\Pi_{i=1}^{n-1}{s_i} x_n = x_1 - \\sum_{i=1}^{n-1}{\\Pi_{j=i+1}^{n-1}{s_j}k_i}\n",
    "$$\n",
    "\n",
    "Note that the Sigma-Pi is not input dependent and hence, it is a constant depending only on the network architecture. This is an important quality, since it makes the computation of the relevant output window an O(1) operation.\n",
    "\n",
    "To deal with boundries, we can write the following formula:\n",
    "$$\n",
    "\\alpha = \\Pi_{i=1}^{n-1}{s_i} \\\\\n",
    "\\beta = \\sum_{i=1}^{n-1}{\\Pi_{j=i+1}^{n-1}{s_j}k_i} \\\\\n",
    "x_n \\ge \\lceil \\frac{x_1 - \\beta}{\\alpha} \\rceil\n",
    "$$\n",
    "where $\\alpha$ and $\\beta$ are constants depending only on the network architecture. Noting that x_n is almost equal to $\\lceil \\frac{x_1 - \\beta}{\\alpha} \\rceil$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dinosr_config import DinosrAudioConfig\n",
    "\n",
    "cfg = DinosrAudioConfig(average_top_k_layers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "def model_creator(cfg):\n",
    "    return TransformerEncoder(cfg)\n",
    "\n",
    "dino_model = DinoSR(cfg, model_creator).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "class ModelPersistantState:\n",
    "    def __init__(self, model_dir):\n",
    "        self.acc_model_path = model_dir + f\"/acc_model.pt\"\n",
    "        self.loss_model_path = model_dir + f\"/loss_model.pt\"\n",
    "        self.performance_path = model_dir + f\"/performance.yaml\"\n",
    "        self.current_step = 0\n",
    "        self.performance = {\n",
    "            \"best_accuracy\": 0,\n",
    "            \"best_loss\": 10000000000,\n",
    "            \"step\": 0,\n",
    "        }\n",
    "\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    def save_model(self, step, model, performance):\n",
    "        if performance['loss'] < self.performance['best_loss']:\n",
    "            self.performance['best_loss'] = performance['loss']\n",
    "            torch.save(model.state_dict(), self.loss_model_path)\n",
    "        if performance['accuracy'] > self.performance['best_accuracy']:\n",
    "            self.performance['best_accuracy'] = performance['accuracy']\n",
    "            torch.save(model.state_dict(), self.acc_model_path)\n",
    "        self.performance['step'] = step\n",
    "        self.performance[f'accuracy_step={step}'] = performance['accuracy']\n",
    "        self.performance[f'loss_step={step}'] = performance['loss']\n",
    "\n",
    "        # Correctly open the file in write mode and dump the YAML\n",
    "        with open(self.performance_path, 'w') as file:\n",
    "            yaml.dump(self.performance, file)\n",
    "        \n",
    "\n",
    "\n",
    "    def load_model(self, model):\n",
    "        model.load_state_dict(torch.load(self.loss_model_path))\n",
    "        with open(self.performance_path, 'r') as file:\n",
    "            self.performance = yaml.load(file, Loader=yaml.FullLoader)\n",
    "        self.current_step = self.performance['step']\n",
    "\n",
    "    def get_current_step(self):\n",
    "        return self.current_step\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dino_model = DinoSR(cfg, model_creator).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model successfully\n"
     ]
    }
   ],
   "source": [
    "model_persistant_state = ModelPersistantState('./models/dino_transformer_model')\n",
    "try:\n",
    "    model_persistant_state.load_model(dino_model)\n",
    "    print(\"loaded model successfully\")\n",
    "except:\n",
    "    print(\"no model to load\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "num_epochs = 50\n",
    "batch_size = 320\n",
    "mini_batch_size = 16\n",
    "learning_rate = 0.0005\n",
    "\n",
    "# Define the learning rate schedule\n",
    "def lr_lambda(initial_step, step):\n",
    "    warmup_steps = 12000\n",
    "    hold_steps = 188000\n",
    "    decay_steps = 200000\n",
    "    initial_lr = 0.0005\n",
    "    final_lr = 0.00005\n",
    "\n",
    "    modified_step = step + initial_step\n",
    "\n",
    "    if modified_step < warmup_steps:\n",
    "        return modified_step / warmup_steps\n",
    "    elif modified_step < warmup_steps + hold_steps:\n",
    "        return 1.0\n",
    "    else:\n",
    "        decay_factor = (modified_step - (warmup_steps + hold_steps)) / decay_steps\n",
    "        return initial_lr * ((final_lr / initial_lr) ** decay_factor) / initial_lr\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(dino_model.parameters(), lr=0.0005)\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda step: lr_lambda(model_persistant_state.get_current_step(), step))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = get_dataloader(mini_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     17\u001b[0m epoch_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (waveforms, lengths) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrainset\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     19\u001b[0m     step \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m*\u001b[39m total_step \u001b[38;5;241m+\u001b[39m i  \u001b[38;5;66;03m# Calculate the current step\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:438\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:386\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1039\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1032\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1039\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/context.py:288\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "# Function to save training history to a YAML file\n",
    "def save_training_history(history, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        yaml.dump(history, file)\n",
    "\n",
    "total_step = len(trainset)\n",
    "n = batch_size // mini_batch_size  # Update parameters every n batches\n",
    "\n",
    "batch_step = model_persistant_state.get_current_step()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_accuracy = 0.0\n",
    "    for i, (waveforms, lengths) in enumerate(trainset):\n",
    "        step = epoch * total_step + i  # Calculate the current step\n",
    "\n",
    "        # Forward pass\n",
    "        results = dino_model(waveforms, lengths)\n",
    "        loss = results['loss'] / n\n",
    "        accuracy = results['accuracy']\n",
    "        \n",
    "        # Accumulate loss and accuracy\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_accuracy += accuracy.item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Accumulate gradients and update parameters\n",
    "        if (i + 1) % n == 0 or (i + 1) == total_step:\n",
    "            optimizer.step()\n",
    "            dino_model.update_teacher_params()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Step the scheduler\n",
    "            scheduler.step()\n",
    "\n",
    "            # Increment the batch step\n",
    "            batch_step += 1\n",
    "\n",
    "            # Save the model and training history\n",
    "            model_persistant_state.save_model(\n",
    "                step=batch_step,\n",
    "                model=dino_model,\n",
    "                performance={\n",
    "                    'loss': epoch_loss / (i + 1),\n",
    "                    'accuracy': epoch_accuracy / (i + 1)\n",
    "                }\n",
    "            )            \n",
    "\n",
    "            print(f'\\rEpoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{total_step}], Loss: {epoch_loss / (i + 1):.4f}, Accuracy: {100 * epoch_accuracy / (i + 1):.2f}% local accuracy is {100 * accuracy:.2f}%, and local loss is: {loss:.4f}', end='', flush=True)\n",
    "\n",
    "    # Calculate and print cumulative loss and accuracy for the epoch\n",
    "    avg_loss = epoch_loss / total_step\n",
    "    avg_accuracy = epoch_accuracy / total_step\n",
    "    print(f'\\nEpoch [{epoch + 1}/{num_epochs}] Summary: Avg Loss: {avg_loss:.4f}, Avg Accuracy: {100 * avg_accuracy:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
