{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from mamba_ssm import Mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionEncoderMambaBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    VisionMambaBlock is a module that implements the Mamba block from the paper\n",
    "    Vision Mamba: Efficient Visual Representation Learning with Bidirectional\n",
    "    State Space Model\n",
    "\n",
    "    Args:\n",
    "        dim (int): The input dimension of the input tensor.\n",
    "        dt_rank (int): The rank of the state space model.\n",
    "        dim_inner (int): The dimension of the inner layer of the\n",
    "            multi-head attention.\n",
    "        d_state (int): The dimension of the state space model.\n",
    "\n",
    "\n",
    "    Example:\n",
    "    >>> block = VisionMambaBlock(dim=256, heads=8, dt_rank=32,\n",
    "            dim_inner=512, d_state=256)\n",
    "    >>> x = torch.randn(1, 32, 256)\n",
    "    >>> out = block(x)\n",
    "    >>> out.shape\n",
    "    torch.Size([1, 32, 256])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        dt_rank: int,\n",
    "        dim_inner: int,\n",
    "        d_state: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.dt_rank = dt_rank\n",
    "        self.dim_inner = dim_inner\n",
    "        self.d_state = d_state\n",
    "\n",
    "        self.forward_conv1d = nn.Conv1d(\n",
    "            in_channels=dim, out_channels=dim, kernel_size=1\n",
    "        )\n",
    "        self.backward_conv1d = nn.Conv1d(\n",
    "            in_channels=dim, out_channels=dim, kernel_size=1\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.ssm = SSM(dim, dt_rank, dim_inner, d_state)\n",
    "\n",
    "        # Linear layer for z and x\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "        # Softplus\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        b, s, d = x.shape\n",
    "\n",
    "        # Skip connection\n",
    "        skip = x\n",
    "\n",
    "        # Normalization\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # Split x into x1 and x2 with linears\n",
    "        z1 = self.proj(x)\n",
    "        x = self.proj(x)\n",
    "\n",
    "        # forward con1d\n",
    "        x1 = self.process_direction(\n",
    "            x,\n",
    "            self.forward_conv1d,\n",
    "            self.ssm,\n",
    "        )\n",
    "\n",
    "        # backward conv1d\n",
    "        x2 = self.process_direction(\n",
    "            x,\n",
    "            self.backward_conv1d,\n",
    "            self.ssm,\n",
    "        )\n",
    "\n",
    "        # Activation\n",
    "        z = self.silu(z1)\n",
    "\n",
    "        # Matmul\n",
    "        x1 *= z\n",
    "        x2 *= z\n",
    "\n",
    "        # Residual connection\n",
    "        return x1 + x2 + skip\n",
    "\n",
    "    def process_direction(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        conv1d: nn.Conv1d,\n",
    "        ssm: SSM,\n",
    "    ):\n",
    "        x = rearrange(x, \"b s d -> b d s\")\n",
    "        x = self.softplus(conv1d(x))\n",
    "        print(f\"Conv1d: {x}\")\n",
    "        x = rearrange(x, \"b d s -> b s d\")\n",
    "        x = ssm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Vim(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Mamba (Vim) model implementation.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Dimension of the model.\n",
    "        dt_rank (int, optional): Rank of the dynamic tensor. Defaults to 32.\n",
    "        dim_inner (int, optional): Inner dimension of the model. Defaults to None.\n",
    "        d_state (int, optional): State dimension of the model. Defaults to None.\n",
    "        num_classes (int, optional): Number of output classes. Defaults to None.\n",
    "        image_size (int, optional): Size of the input image. Defaults to 224.\n",
    "        patch_size (int, optional): Size of the image patch. Defaults to 16.\n",
    "        channels (int, optional): Number of image channels. Defaults to 3.\n",
    "        dropout (float, optional): Dropout rate. Defaults to 0.1.\n",
    "        depth (int, optional): Number of encoder layers. Defaults to 12.\n",
    "\n",
    "    Attributes:\n",
    "        dim (int): Dimension of the model.\n",
    "        dt_rank (int): Rank of the dynamic tensor.\n",
    "        dim_inner (int): Inner dimension of the model.\n",
    "        d_state (int): State dimension of the model.\n",
    "        num_classes (int): Number of output classes.\n",
    "        image_size (int): Size of the input image.\n",
    "        patch_size (int): Size of the image patch.\n",
    "        channels (int): Number of image channels.\n",
    "        dropout (float): Dropout rate.\n",
    "        depth (int): Number of encoder layers.\n",
    "        to_patch_embedding (nn.Sequential): Sequential module for patch embedding.\n",
    "        dropout (nn.Dropout): Dropout module.\n",
    "        cls_token (nn.Parameter): Class token parameter.\n",
    "        to_latent (nn.Identity): Identity module for latent representation.\n",
    "        layers (nn.ModuleList): List of encoder layers.\n",
    "        output_head (output_head): Output head module.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        dt_rank: int = 32,\n",
    "        dim_inner: int = None,\n",
    "        d_state: int = None,\n",
    "        num_classes: int = None,\n",
    "        image_size: int = 224,\n",
    "        patch_size: int = 16,\n",
    "        channels: int = 3,\n",
    "        dropout: float = 0.1,\n",
    "        depth: int = 12,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.dt_rank = dt_rank\n",
    "        self.dim_inner = dim_inner\n",
    "        self.d_state = d_state\n",
    "        self.num_classes = num_classes\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.channels = channels\n",
    "        self.dropout = dropout\n",
    "        self.depth = depth\n",
    "\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange(\n",
    "                \"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\",\n",
    "                p1=patch_height,\n",
    "                p2=patch_height,\n",
    "            ),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # class token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "\n",
    "        # Latent\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        # encoder layers\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # Append the encoder layers\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(\n",
    "                VisionEncoderMambaBlock(\n",
    "                    dim=dim,\n",
    "                    dt_rank=dt_rank,\n",
    "                    dim_inner=dim_inner,\n",
    "                    d_state=d_state,\n",
    "                    *args,\n",
    "                    **kwargs,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Output head\n",
    "        self.output_head = output_head(dim, num_classes)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        # Patch embedding\n",
    "        b, c, h, w = x.shape\n",
    "\n",
    "        x = self.to_patch_embedding(x)\n",
    "        print(f\"Patch embedding: {x.shape}\")\n",
    "\n",
    "        # Shape\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        # Cls tokens\n",
    "        cls_tokens = repeat(self.cls_token, \"() n d -> b n d\", b=b)\n",
    "        print(f\"Cls tokens: {cls_tokens.shape}\")\n",
    "\n",
    "        # Concatenate\n",
    "        # x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # Dropout\n",
    "        x = self.dropout(x)\n",
    "        print(x.shape)\n",
    "\n",
    "        # Forward pass with the layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            print(f\"Layer: {x.shape}\")\n",
    "\n",
    "        # Latent\n",
    "        x = self.to_latent(x)\n",
    "\n",
    "        # x = reduce(x, \"b s d -> b d\", \"mean\")\n",
    "\n",
    "        # Output head with the cls tokens\n",
    "        return self.output_head(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
