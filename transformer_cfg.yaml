_name: null
activation_dropout: 0.0
activation_fn: gelu
attention_dropout: 0.1
attn_type: ''
average_top_k_layers: 4
batch_norm_target_layer: false
checkpoint_activations: false
codebook_end_decay: 0.9
codebook_end_decay_step: 0
codebook_init_decay: 0.9
codebook_negatives: 0
codebook_size: 256
conv_bias: false
conv_feature_layers: '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]'
conv_pos: 128
conv_pos_groups: 16
crop_seq_to_multiple: 1
cross_sample_negatives: 0
depthwise_conv_kernel_size: 31
discrete: false
dropout: 0.1
dropout_features: 0.0
dropout_input: 0.0
ema_anneal_end_step: 30000
ema_decay: 0.999
ema_end_decay: 0.9999
ema_layers_only: true
ema_transformer_only: true
encoder_attention_heads: 8
encoder_embed_dim: 256
encoder_ffn_embed_dim: 512
encoder_layerdrop: 0.0
encoder_layers: 6
extractor_mode: layer_norm
feature_grad_mult: 1.0
final_dim: 0
fp16: false
freeze_pre_enc_modules: true
freeze_teacher_step: 200001
group_norm_target_layer: false
instance_norm_target_layer: false
instance_norm_targets: false
latent_dim: 0
latent_groups: 2
latent_temp:
- 2
- 0.5
- 0.999995
latent_vars: 320
layer_norm_first: false
layer_norm_target_layer: false
layer_norm_targets: false
layer_type: transformer
logit_temp: 0.1
loss_beta: 0
loss_scale: null
mamba_d_state: 64
mask_channel_before: false
mask_channel_length: 10
mask_channel_min_space: 1
mask_channel_other: 0
mask_channel_prob: 0.0
mask_channel_selection: static
mask_dropout: 0.0
mask_length: 10
mask_min_space: 1
mask_other: 0
mask_prob: 0.65
mask_selection: static
max_positions: 100000
max_update: ${optimization.max_update}
min_pred_var: 0.01
min_target_var: 0.1
model_type: transformer
negatives_from_everywhere: false
no_mask_channel_overlap: false
no_mask_overlap: false
normal_init_codebook: false
num_negatives: 100
pos_conv_depth: 1
pos_enc_type: abs
quantize_input: false
quantize_targets: false
quantizer_depth: 1
quantizer_factor: 3
require_same_masks: true
required_seq_len_multiple: 2
same_quantizer: false
target_glu: false
